# 主从复制

![image-20230811083606295](D:\typora\Golang_Engineer\typora-user-images\image-20230811083606295.png)

多台服务器如何保持数据一致性，数据的读写操作是否每台服务器都可以处理？Redis提供主从复制，它可以保证多台服务器的数据一致性，且主从服务器之间采用的是读写分离的方式。

 主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受从服务器同步过来写操作命令，然后执行这条命令

![image-20230811202150339](D:\typora\Golang_Engineer\typora-user-images\image-20230811202150339.png)

也就是说，所有的数据修改只在主服务器上进行，然后将最新的数据同步给从服务器，这样就使得主从服务器的数据是一致的。

## Redis主从复制如何配置

在Redis中配置主从复制（Replication）是一种常见的方式，用于创建一个主节点（Master）和一个或多个从节点（Slave）之间的数据复制关系，以实现数据的备份、负载均衡和高可用性等需求。以下是配置Redis主从复制的基本步骤：

1. 打开Redis配置文件： 找到Redis的配置文件，通常位于Redis安装目录下的`redis.conf`文件。你可以使用文本编辑器打开此文件。

2. 配置主节点： 在Redis配置文件中，找到并编辑以下设置以配置主节点：

   ```
   perlCopy code# 启用主节点
   port 6379  # 默认端口号，你也可以使用其他端口
   
   # 设置主节点密码（可选）
   requirepass your_password_here
   
   # 允许其他节点连接到此主节点
   bind 0.0.0.0
   
   # 启用主从复制
   replicaof no one
   ```

   确保将`replicaof`设置为`no one`，因为这是主节点。

3. 配置从节点： 在Redis配置文件中，找到并编辑以下设置以配置从节点：

   ```
   yamlCopy code# 启用从节点
   port 6380  # 使用不同的端口号
   
   # 设置从节点密码（可选）
   requirepass your_password_here
   
   # 指定主节点的IP地址和端口
   replicaof your_master_ip 6379
   ```

   将`your_master_ip`替换为你主节点的IP地址。

4. 保存配置文件并重启Redis： 保存Redis配置文件后，关闭并重新启动Redis，以使更改生效。

5. 验证主从复制是否正常工作： 使用以下命令验证主从复制是否已经建立：

   ```
   cssCopy code
   redis-cli -h your_slave_ip -p 6380
   ```

   在从节点上执行上述命令，将`your_slave_ip`替换为从节点的IP地址。连接到从节点后，运行`INFO replication`命令，它将显示有关主从复制状态的信息。

主从复制已经成功配置完成。现在，主节点上的数据将自动复制到从节点，从节点将按照主节点的数据变化进行同步。你可以随时添加更多的从节点来扩展复制集群。确保维护好安全性，使用密码保护主从节点以及限制访问权限，以防止未经授权的访问。

## 第一次同步

多台服务器之间通过replicaof（Redis5.0之前使用slaveof）命令形成主服务器和从服务器的关系

比如，现在有服务器 A 和 服务器 B，我们在服务器 B 上执行下面这条命令：

```text
# 服务器 B 执行这条命令
replicaof <服务器 A 的 IP 地址> <服务器 A 的 Redis 端口号>
```

主从服务器间的第一次同步的过程可分为三个阶段：

- 第一阶段是建立链接、协商同步；
- 第二阶段是主服务器同步数据给从服务器；
- 第三阶段是主服务器发送新写操作命令给从服务器。

## 命令传播

主从服务器在完成第一次同步后，双方之间就会维护一个TCP连接。

![image-20230811202903730](D:\typora\Golang_Engineer\typora-user-images\image-20230811202903730.png)

后续主服务器可以通过这个连接继续将写操作命令传播给从服务器，然后从服务器执行该命令，使得与主服务器的数据库状态相同。而且这个连接是长连接的，目的是避免频繁的 TCP 连接和断开带来的性能开销。上面的这个过程被称为**基于长连接的命令传播**，通过这种方式来保证第一次同步后的主从服务器的数据一致性。

## 分担主服务器压力

从服务器可以有自己的从服务器，我们可以把拥有从服务器的从服务器当作经理角色，它不仅可以接收主服务器的同步数据，自己也可以同时作为主服务器的形式将数据同步给从服务器，组织形式如下图：

![image-20230811203109889](D:\typora\Golang_Engineer\typora-user-images\image-20230811203109889.png)

通过这种方式，**主服务器生成 RDB 和传输 RDB 的压力可以分摊到充当经理角色的从服务器**。

那具体怎么做到的呢？其实很简单，我们在「从服务器」上执行下面这条命令，使其作为目标服务器的从服务器：

```text
replicaof <目标服务器的IP> 6379
```

此时如果目标服务器本身也是「从服务器」，那么该目标服务器就会成为「经理」的角色，不仅可以接受主服务器同步的数据，也会把数据同步给自己旗下的从服务器，从而减轻主服务器的负担。

## 增量复制

在 Redis 2.8 之前，如果主从服务器在命令同步时出现了网络断开又恢复的情况，从服务器就会和主服务器重新进行一次**全量复制**，很明显这样的开销太大了，必须要改进一波。

所以，从 Redis 2.8 开始，网络断开又恢复后，从主从服务器会采用**增量复制**的方式继续同步，也就是只会把网络断开期间主服务器接收到的写操作命令，同步给从服务器。

![image-20230811203255146](D:\typora\Golang_Engineer\typora-user-images\image-20230811203255146.png)

主要有三个步骤：

- 从服务器在恢复网络后，会发送 psync 命令给主服务器，此时的 psync 命令里的 offset 参数不是 -1；
- 主服务器收到该命令后，然后用 CONTINUE 响应命令告诉从服务器接下来采用增量复制的方式同步数据；
- 然后主服务将主从服务器断线期间，所执行的写命令发送给从服务器，然后从服务器执行这些命令。

## redis主从复制原理

Redis主从复制是一种常见的数据复制机制，用于将一个Redis主节点的数据复制到多个从节点上。主节点负责写操作，而从节点负责读操作。下面是Redis主从复制的基本原理和步骤：

1. 建立连接：从节点与主节点建立连接，并发送复制请求。从节点通过发送SYNC命令请求全量复制或通过发送PSYNC命令请求部分复制。

2. 全量复制（如果是首次复制）：主节点收到全量复制请求后，会执行BGSAVE命令生成RDB快照文件，将快照文件发送给从节点。从节点接收到快照文件后，会加载快照文件中的数据，建立起与主节点相同的数据集。

3. 部分复制（如果是增量复制）：主节点收到部分复制请求后，会将当前的数据集转换成一个内存数据流，通过流的方式发送给从节点。从节点接收到数据流后，将数据应用到自己的数据集上，使得数据与主节点保持一致。

4. 命令传播：一旦复制建立完成，主节点会将执行的写命令发送给所有的从节点。从节点接收到命令后，按照相同的顺序执行这些写命令，从而保持数据的一致性。

5. 增量复制：主节点将每次执行的写命令通过网络传输给从节点，从节点接收到写命令后，执行相同的操作。这样可以保持从节点与主节点的数据同步。

6. 故障转移：如果主节点发生故障，无法提供服务，从节点可以选举一个新的主节点，重新进行主从复制。选举过程中，从节点会选择一个复制偏移量（replication offset）最大的节点作为新的主节点，并将其余节点设置为新的从节点。

Redis主从复制的优点在于可以提高读取性能和数据的可用性。从节点可以处理读请求，减轻主节点的负载，并提供高可用性，因为如果主节点发生故障，可以将一个从节点升级为新的主节点。主从复制在分布式缓存、数据备份和负载均衡等场景中得到广泛应用。

## 总结

主从复制共有三种模式：**全量复制、基于长连接的命令传播、增量复制**。

主从服务器第一次同步的时候，就是采用全量复制，此时主服务器会两个耗时的地方，分别是生成 RDB 文件和传输 RDB 文件。为了避免过多的从服务器和主服务器进行全量复制，可以把一部分从服务器升级为「经理角色」，让它也有自己的从服务器，通过这样可以分摊主服务器的压力。

第一次同步完成后，主从服务器都会维护着一个长连接，主服务器在接收到写操作命令后，就会通过这个连接将写命令传播给从服务器，来保证主从服务器的数据一致性。

如果遇到网络断开，增量复制就可以上场了，不过这个还跟 repl_backlog_size 这个大小有关系。

如果它配置的过小，主从服务器网络恢复时，可能发生「从服务器」想读的数据已经被覆盖了，那么这时就会导致主服务器采用全量复制的方式。所以为了避免这种情况的频繁发生，要调大这个参数的值，以降低主从服务器断开后全量同步的概率。

## 面试题

### 主从复制

Redis的主从数据是异步同步的，所以分布式的Redis系统并不满足一致性要求，但Redis满足最终一致性，从节点会努力追赶主节点，最终从节点的状态会和主节点的状态保持一致。Redis满足可用性。

增量同步：修改状态的操作记录到本地内存buffer中，然后异步将buffer中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一样的状态，一边向主节点反馈自己同步到哪里了（偏移量）。Redis的复制内存buffer是一个定长的环形数组，如果数组内容慢了，就会从头开始覆盖前面的内容。如果存在没有同步的指令被后续的指令覆盖掉了，需要使用到更加复杂的同步机制—快照同步。

快照同步：首先需要在主节点上进行一次bgsave，将当前内存数据全部快照到磁盘文件中，然后再将快照文件的内容全部传送到从节点。从节点将快照文件接收完毕后，立即执行一次全量加载，加载之前先要将当前内存的数据清空，加载完毕之后通知主节点继续进行增量同步。避免快照同步死循环方法：配置合适的复制buffer大小参数。

Redis复制是异步进行的，wait指令可以让异步复制变身同步复制，确保系统的强一致性（不严格）。wait提供两个参数，第一个参数是从节点的数量N，第二个参数是时间t，以毫秒为单位。两个参数的含义是：等待wait指令之前的所有写操作同步到N个从节点（也就是确保N个从节点的同步没有滞后），最多等待时间为t。如果时间t=0，表示无限等待直到N个从节点同步完成。

### 强一致性和弱一致性

一致性是指多个系统或组件在执行相同操作时，所得到的结果是否相同的程度。强一致性和弱一致性是一致性的两种不同级别：

强一致性：在这种情况下，所有系统或组件在执行相同操作时，都会立即得到相同的结果。这种情况下，无论在任何时候查询，系统都会返回最新的结果。

弱一致性：在这种情况下，系统或组件在执行相同操作时，可能会得到不同的结果。这种情况下，系统可能会返回旧的结果，直到所有系统都更新了数据。

强一致性通常用于对数据准确性要求非常高的场景，例如金融交易系统。而弱一致性则通常用于对数据实时性要求不高的场景，例如社交媒体应用程序。在实际应用中，选择强一致性还是弱一致性取决于具体的业务需求和系统设计。

### 什么是Redis主从复制

- Redis主从复制是指将一个Redis服务器（主节点）的数据复制到其他Redis服务器（从节点）上的过程。主节点负责写操作，而从节点负责读操作，从而实现数据的备份、读写分离和负载均衡。

### Redis主从复制的原理

- 主从复制的原理是通过Redis的发布/订阅机制实现的。当主节点有写操作时，会将写操作的命令传播给所有从节点，从节点会执行相同的命令来达到数据同步的目的。

1. **建立连接：**
   - 从服务器通过向主服务器发送SYNC命令来发起复制。
   - 主服务器收到SYNC命令后开始执行BGSAVE命令，创建一个RDB快照文件（持久化文件）。
   - 主服务器将快照文件保存到磁盘，并在此期间将写命令缓存到内存缓冲区。
2. **传送快照：**
   - 一旦快照文件创建完毕，主服务器开始将快照文件发送给从服务器。
   - 从服务器接收到快照文件后，将其保存到本地磁盘。
3. **增量同步：**
   - 主服务器继续将内存缓冲区中的写命令追加到快照文件之后，发送给从服务器。
   - 从服务器接收到增量数据，并将其应用到本地数据库，保持与主服务器相同的数据状态。
4. **持续同步：**
   - 从服务器会保持与主服务器的连接，并在收到主服务器的更新时，立即将这些更新应用到本地数据库，以保持数据的同步。
5. **处理断开连接和重新连接：**
   - 如果从服务器与主服务器的连接断开，从服务器会尝试重新连接。
   - 当重新连接时，从服务器可以选择重新进行完整的同步或者从上次同步的位置开始进行增量同步，这取决于是否有持久化文件。
6. **复制的配置：**
   - Redis支持配置主从复制的方式，可以配置从服务器只读，或者允许从服务器进行读写操作。
   - 通过配置文件或者命令行可以设置主从服务器的关系，以及是否允许从服务器成为其他从服务器的主服务器。

### Redis主从复制的优点

- 数据备份：主节点的数据会被复制到从节点上，当主节点发生故障时，从节点可以接管服务，保证数据的可用性。
- 读写分离：主节点负责写操作，从节点负责读操作，可以提高系统的读写性能。
- 负载均衡：可以通过增加从节点来分担主节点的负载，提高系统的并发能力。

### Redis主从复制的缺点

1. 数据延迟：由于主从复制是异步的过程，主节点的写操作需要经过网络传输到从节点，因此从节点上的数据可能会有一定的延迟。这意味着如果在主节点写入数据后立即进行读操作，可能读取到的是旧数据。
2. 单点故障：虽然主从复制可以提高系统的可用性，但是主节点仍然是一个单点故障。当主节点发生故障时，需要手动将一个从节点升级为主节点，这个过程可能会导致一段时间的服务不可用。
3. 无法实现自动故障转移：Redis主从复制需要手动进行故障转移，即将一个从节点升级为主节点，这个过程需要人工干预。在高负载的情况下，可能会导致一段时间的服务不可用。
4. 无法保证数据一致性：由于主从复制是异步的过程，当主节点发生故障时，从节点上的数据可能会滞后于主节点。在这种情况下，如果没有进行数据同步的操作，可能会导致数据不一致的问题。
5. 配置复杂：Redis主从复制的配置相对复杂，需要在主节点和从节点上进行配置，并确保配置的正确性。如果配置不正确，可能会导致主从复制失败或者数据不一致的问题。

### Redis主从复制配置步骤

- 配置主节点：在主节点的配置文件中设置`slaveof`参数为空，启动主节点。
- 配置从节点：在从节点的配置文件中设置`slaveof`参数为主节点的地址和端口号，启动从节点。
- 验证主从复制：可以通过在主节点上写入数据，然后在从节点上读取数据来验证主从复制是否成功。

### Redis主从复制同步策略

- Redis主从复制有两种同步策略：全量复制和增量复制。
- 全量复制：当从节点与主节点建立连接时，主节点会将所有数据发送给从节点，从节点接收完所有数据后才能开始处理客户端的读请求。
- 增量复制：当从节点与主节点建立连接后，主节点会将写操作的命令发送给从节点，从节点执行相同的命令来达到数据同步的目的。

### 主从节点是长还是短连接

长连接

### 判断节点是否正常工作

Redis 判断节点是否正常工作，基本都是通过互相的 ping-pong 心态检测机制，如果有一半以上的节点去 ping 一个节点的时候没有 pong 回应，集群就会认为这个节点挂掉了，会断开与这个节点的连接。Redis 主从节点发送的心态间隔是不一样的，而且作用也有一点区别：

1. Redis 主节点默认每隔 10 秒对从节点发送 ping 命令，判断从节点的存活性和连接状态，可通过参数repl-ping-slave-period控制发送频率。
2. Redis 从节点每隔 1 秒发送 replconf ack{offset} 命令，给主节点上报自身当前的复制偏移量，目的是为了：

- 实时监测主从节点网络状态；
- 上报自身复制偏移量， 检查复制数据是否丢失， 如果从节点数据丢失， 再从主节点的复制缓冲区中拉取丢失数据。

### 主从复制架构中，过期key如何处理

主节点处理了一个key或者通过淘汰算法淘汰了一个key，这个时间主节点模拟一条del命令发送给从节点，从节点收到该命令后，就进行删除key的操作。

### Redis 是同步复制还是异步复制

Redis 主节点每次收到写命令之后，先写到内部的缓冲区，然后异步发送给从节点。

### 主从复制中两个 Buffer(replication buffer 、repl backlog buffer)有什么区别？

replication buffer 、repl backlog buffer 区别如下：

- 出现的阶段不一样：
  - repl backlog buffer 是在增量复制阶段出现，**一个主节点只分配一个 repl backlog buffer**；
  - replication buffer 是在全量复制阶段和增量复制阶段都会出现，**主节点会给每个新连接的从节点，分配一个 replication buffer**；
- 这两个 Buffer 都有大小限制的，当缓冲区满了之后，发生的事情不一样：
  - 当 repl backlog buffer 满了，因为是环形结构，会直接**覆盖起始位置数据**;
  - 当 replication buffer 满了，会导致连接断开，删除缓存，从节点重新连接，**重新开始全量复制**。

### 如何应对主从数据不一致

会出现主从数据不一致的现象，是**因为主从节点间的命令复制是异步进行的**，所以无法实现强一致性保证(主从数据时时刻刻保持一致)

第一种方法，尽量保证主从节点间的网络连接状况良好，避免主从节点在不同的机房。

第二种方法，可以开发一个外部程序来监控主从节点间的复制进度。具体做法：

- Redis 的 INFO replication 命令可以查看主节点接收写命令的进度信息（master_repl_offset）和从节点复制写命令的进度信息（slave_repl_offset），所以，我们就可以开发一个监控程序，先用 INFO replication 命令查到主、从节点的进度，然后，我们用 master_repl_offset 减去 slave_repl_offset，这样就能得到从节点和主节点间的复制进度差值了。
- 如果某个从节点的进度差值大于我们预设的阈值，我们可以让客户端不再和这个从节点连接进行数据读取，这样就可以减少读到不一致数据的情况。不过，为了避免出现客户端和所有从节点都不能连接的情况，我们需要把复制进度差值的阈值设置得大一些。

### 主从切换如何减少数据丢失

主从切换过程中，产生数据丢失的情况有两种：

- 异步复制同步丢失

对于 Redis 主节点与从节点之间的数据复制，是异步复制的，当客户端发送写请求给主节点的时候，客户端会返回 ok，接着主节点将写请求异步同步给各个从节点，但是如果此时主节点还没来得及同步给从节点时发生了断电，那么主节点内存中的数据会丢失。

**解决方案：**

Redis 配置里有一个参数 min-slaves-max-lag，表示一旦所有的从节点数据复制和同步的延迟都超过了 min-slaves-max-lag 定义的值，那么主节点就会拒绝接收任何请求。

假设将 min-slaves-max-lag 配置为 10s 后，根据目前 master->slave 的复制速度，如果数据同步完成所需要时间超过10s，就会认为 master 未来宕机后损失的数据会很多，master 就拒绝写入新请求。这样就能将 master 和 slave 数据差控制在10s内，即使 master 宕机也只是这未复制的 10s 数据。

那么对于客户端，当客户端发现 master 不可写后，我们可以采取降级措施，将数据暂时写入本地缓存和磁盘中，在一段时间（等 master 恢复正常）后重新写入 master 来保证数据不丢失，也可以将数据写入 kafka 消息队列，等 master 恢复正常，再隔一段时间去消费 kafka 中的数据，让将数据重新写入 master。

- 集群产生脑裂数据丢失

由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。

**解决方案**：

当主节点发现「从节点下线的数量太多」，或者「网络延迟太大」的时候，那么主节点会禁止写操作，直接把错误返回给客户端。在 Redis 的配置文件中有两个参数我们可以设置：

- min-slaves-to-write x，主节点必须要有**至少 x 个从节点连接**，如果小于这个数，主节点会禁止写数据。
- min-slaves-max-lag x，主从数据复制和同步的延迟**不能超过 x 秒**，如果主从同步的延迟超过 x 秒，主节点会禁止写数据。

我们可以把 min-slaves-to-write 和 min-slaves-max-lag 这两个配置项搭配起来使用，分别给它们设置一定的阈值，假设为 N 和 T。这两个配置项组合后的要求是，**主节点连接的从节点中至少有 N 个从节点，「并且」主节点进行数据复制时的 ACK 消息延迟不能超过 T 秒**，否则，主节点就不会再接收客户端的写请求了。

即使原主节点是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从节点进行同步，自然也就无法和从节点进行 ACK 确认了。这样一来，min-slaves-to-write 和 min-slaves-max-lag 的组合要求就无法得到满足，**原主节点就会被限制接收客户端写请求，客户端也就不能在原主节点中写入新数据了**。

**等到新主节点上线时，就只有新主节点能接收和处理客户端请求，此时，新写的数据会被直接写到新主节点中。而原主节点会被哨兵降为从节点，即使它的数据被清空了，也不会有新数据丢失。我再来给你举个例子。**

假设我们将 min-slaves-to-write 设置为 1，把 min-slaves-max-lag 设置为 12s，把哨兵的 down-after-milliseconds 设置为 10s，主节点因为某些原因卡住了 15s，导致哨兵判断主节点客观下线，开始进行主从切换。同时，因为原主节点卡住了 15s，没有一个从节点能和原主节点在 12s 内进行数据复制，原主节点也无法接收客户端请求了。这样一来，主从切换完成后，也只有新主节点能接收请求，不会发生脑裂，也就不会发生数据丢失的问题了。

### 主从如何做到故障自动切换

主节点挂了 ，从节点是无法自动升级为主节点的，这个过程需要人工处理，在此期间 Redis 无法对外提供写操作。此时，Redis 哨兵机制就登场了，哨兵在发现主节点出现故障时，由哨兵自动完成故障发现和故障转移，并通知给应用方，从而实现高可用性。

# 哨兵模式

## 为什么要有哨兵模式

主从模式采用读写分离，如果主节点挂了，将没有主节点来服务客户端请求，如果要恢复服务时需要人工介入，太不智能了。Redis 在 2.8 版本以后提供的**哨兵（\*Sentinel\*）机制**，它的作用是实现**主从节点故障转移**。它会监测主节点是否存活，如果发现主节点挂了，它就会选举一个从节点切换为主节点，并且把新主节点的相关信息通知给从节点和客户端。

##  哨兵机制如何工作

哨兵其实是一个运行在特殊模式下的 Redis 进程，所以它也是一个节点，哨兵节点主要负责三件事：监控、选主、通知

## 如何判断主节点是真的故障

哨兵会每隔 1 秒给所有主从节点发送 PING 命令，当主从节点收到 PING 命令后，会发送一个响应命令给哨兵，这样就可以判断它们是否在正常运行。

![image-20230811210434456](D:\typora\Golang_Engineer\typora-user-images\image-20230811210434456.png)

如果主节点或者从节点没有在规定的时间内响应哨兵的 PING 命令，哨兵就会将它们标记为「**主观下线**」。这个「规定的时间」是配置项 `down-after-milliseconds` 参数设定的，单位是毫秒。

### 主观下线，客观下线

客观下线只适用于主节点。之所以针对「主节点」设计「主观下线」和「客观下线」两个状态，是因为有可能「主节点」其实并没有故障，可能只是因为主节点的系统压力比较大或者网络发送了拥塞，导致主节点没有在规定时间内响应哨兵的 PING 命令。

所以，为了减少误判的情况，哨兵在部署的时候不会只部署一个节点，而是用多个节点部署成**哨兵集群**（*最少需要三台机器来部署哨兵集群*），**通过多个哨兵节点一起判断，就可以就可以避免单个哨兵因为自身网络状况不好，而误判主节点下线的情况**。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。

### 具体怎么判定为客观下线

当一个哨兵判断主节点为「主观下线」后，就会向其他哨兵发起命令，其他哨兵收到这个命令后，就会根据自身和主节点的网络状况，做出赞成投票或者拒绝投票的响应。

![image-20230811210904951](D:\typora\Golang_Engineer\typora-user-images\image-20230811210904951.png)

当这个哨兵的赞同票数达到哨兵配置文件中的 quorum 配置项设定的值后，这时主节点就会被该哨兵标记为「客观下线」。例如，现在有 3 个哨兵，quorum 配置的是 2，那么一个哨兵需要 2 张赞成票，就可以标记主节点为“客观下线”了。这 2 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。PS：quorum 的值一般设置为哨兵个数的二分之一加1，例如 3 个哨兵就设置 2。哨兵判断完主节点客观下线后，哨兵就要开始在多个「从节点」中，选出一个从节点来做新主节点。

## 哪个哨兵进行主从故障转移

**哨兵是以哨兵集群的方式存在的**。需要在哨兵集群中选出一个 leader，让 leader 来执行主从切换。选举 leader 的过程其实是一个投票的过程，在投票开始前，肯定得有个「候选者」。哪个哨兵节点判断主节点为「客观下线」，这个哨兵节点就是候选者，所谓的候选者就是想当 Leader 的哨兵。

### 候选者如何成为 Leader

候选者会向其他哨兵发送命令，表明希望成为 Leader 来执行主从切换，并让所有其他哨兵对它进行投票。每个哨兵只有一次投票机会，如果用完后就不能参与投票了，可以投给自己或投给别人，但是只有候选者才能把票投给自己。那么在投票过程中，任何一个「候选者」，要满足两个条件：

- 第一，拿到半数以上的赞成票；
- 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。

举个例子，假设哨兵节点有 3 个，quorum 设置为 2，那么任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以选举成功了。如果没有满足条件，就需要重新进行选举。这时候有的同学就会问了，如果某个时间点，刚好有两个哨兵节点判断到主节点为客观下线，那这时不就有两个候选者了？这时该如何决定谁是 Leader 呢？

每位候选者都会先给自己投一票，然后向其他哨兵发起投票请求。如果投票者先收到「候选者 A」的投票请求，就会先投票给它，如果投票者用完投票机会后，收到「候选者 B」的投票请求后，就会拒绝投票。这时，候选者 A 先满足了上面的那两个条件，所以「候选者 A」就会被选举为 Leader。

### why哨兵节点至少有 3 个

如果哨兵集群中只有 2 个哨兵节点，此时如果一个哨兵想要成功成为 Leader，必须获得 2 票，而不是 1 票。所以，如果哨兵集群中有个哨兵挂掉了，那么就只剩一个哨兵了，如果这个哨兵想要成为 Leader，这时票数就没办法达到 2 票，就无法成功成为 Leader，这时是无法进行主从节点切换的。

因此，通常我们至少会配置 3 个哨兵节点。这时，如果哨兵集群中有个哨兵挂掉了，那么还剩下两个个哨兵，如果这个哨兵想要成为 Leader，这时还是有机会达到 2 票的，所以还是可以选举成功的，不会导致无法进行主从节点切换。

当然，你要问，如果 3 个哨兵节点，挂了 2 个怎么办？这个时候得人为介入了，或者增加多一点哨兵节点。再说一个问题，Redis 1 主 4 从，5 个哨兵 ，quorum 设置为 3，如果 2 个哨兵故障，当主节点宕机时，哨兵能否判断主节点“客观下线”？主从能否自动切换？

- **哨兵集群可以判定主节点“客观下线”**。哨兵集群还剩下 3 个哨兵，当一个哨兵判断主节点“主观下线”后，询问另外 2 个哨兵后，有可能能拿到 3 张赞同票，这时就达到了 quorum 的值，因此，哨兵集群可以判定主节点为“客观下线”。
- **哨兵集群可以完成主从切换**。当有个哨兵标记主节点为「客观下线」后，就会进行选举 Leader 的过程，因为此时哨兵集群还剩下 3 个哨兵，那么还是可以拿到半数以上（5/2+1=3）的票，而且也达到了 quorum 值，满足了选举 Leader 的两个条件， 所以就能选举成功，因此哨兵集群可以完成主从切换。

如果 quorum 设置为 2 ，并且如果有 3 个哨兵故障的话。此时哨兵集群还是可以判定主节点为“客观下线”，但是哨兵不能完成主从切换了，大家可以自己推演下。如果 quorum 设置为 3，并且如果有 3 个哨兵故障的话，哨兵集群即不能判定主节点为“客观下线”，也不能完成主从切换了。可以看到，quorum 为 2 的时候，并且如果有 3 个哨兵故障的话，虽然可以判定主节点为“客观下线”，但是不能完成主从切换，这样感觉「判定主节点为客观下线」这件事情白做了一样，既然这样，还不如不要做，quorum 为 3 的时候，就可以避免这种无用功。所以，**quorum 的值建议设置为哨兵个数的二分之一加1**，例如 3 个哨兵就设置 2，5 个哨兵设置为 3，而且**哨兵节点的数量应该是奇数**。

## 主从故障转移的过程

在哨兵集群中通过投票的方式，选举出了哨兵 leader 后，就可以进行主从故障转移的过程了，如下图：

![image-20230811211404610](D:\typora\Golang_Engineer\typora-user-images\image-20230811211404610.png)

主从故障转移操作包含以下四个步骤：

- 第一步：在已下线主节点（旧主节点）属下的所有「从节点」里面，挑选出一个从节点，并将其转换为主节点。
- 第二步：让已下线主节点属下的所有「从节点」修改复制目标，修改为复制「新主节点」；
- 第三步：将新主节点的 IP 地址和信息，通过「发布者/订阅者机制」通知给客户端；
- 第四步：继续监视旧主节点，当这个旧主节点重新上线时，将它设置为新主节点的从节点；

# codis



## one

Codis使用Go语言开发，他是一个中间代理中间件，和Redis一样也使用Redis协议对外提供服务，当客户端向Codis发送指令时，Codis负责将指令转发到后面的Redis实例来执行，并将返回结果再转回给客户端。Codis上挂接的所有Redis实例构成一个Redis集群，当集群空间不足时，可以通过动态增加Redis实例来实现扩容需求。

Codis 是一个基于 Redis 的分布式集群解决方案，旨在解决 Redis 单机容量有限的问题。它通过将数据分片存储在多台 Redis 服务器上，并提供一个代理层来协调客户端的请求，从而实现了高可用性和横向扩展。

下面是 Codis 的主要特点和工作原理的详细介绍：

特点：

1. 分布式存储：Codis 将数据分片存储在多个 Redis 服务器上，每个服务器只负责一部分数据，从而实现了数据的分布式存储。这样可以提高整个集群的容量和吞吐量。
2. 高可用性：Codis 支持主从复制和自动故障转移，当某个 Redis 实例发生故障时，Codis 可以自动将该实例的工作转移到其他正常的实例上，从而保证整个集群的高可用性。
3. 代理层：Codis 提供了一个代理层，客户端通过代理层与整个集群进行交互，代理层负责将请求路由到相应的 Redis 服务器上，并处理一些分片相关的逻辑，例如数据的分片策略和数据迁移。
4. 兼容性：Codis 兼容 Redis 协议，客户端可以使用标准的 Redis 客户端库与 Codis 集群进行交互，无需修改现有的应用程序代码。

工作原理：

1. 数据分片：Codis 使用哈希函数对数据进行分片，将数据均匀地分布在多个 Redis 服务器上，每个服务器负责一部分数据。通常情况下，Codis 使用的哈希函数是基于键的，保证相同键的数据被分配到同一个 Redis 服务器上。
2. 代理路由：客户端通过与 Codis 的代理层进行通信来访问数据。代理层接收客户端的请求后，根据请求的键使用相同的哈希函数计算出数据所在的 Redis 服务器，并将请求路由到相应的服务器上。
3. 分片策略：Codis 支持多种分片策略，例如哈希分片和范围分片。哈希分片将键的哈希值映射到一个哈希环上，并根据环上的位置确定数据所在的 Redis 服务器。范围分片将键的取值范围划分为多个区间，并将每个区间映射到一个 Redis 服务器上。
4. 数据迁移：当需要扩展集群或者调整分片策略时，Codis 可以自动进行数据迁移。数据迁移过程中，Codis 会将数据从一个 Redis 服务器复制到另一个服务器，保证数据的一致性和可用性。

总之，Codis 是一个基于 Redis 的分布式集群解决方案，通过数据分片和代理层来实现高可用性和横向扩展。它可以提高 Redis 的容量和吞吐量，并且对客户端是透明的，兼容标准的 Redis 协议。

## Codis分片原理

Codis默认将所有key划分为1024个槽位，它首先对客户端传过来的key进行crc32运算计算hash值，再将hash后的整数值对1024这个整数行取模得到一个余数，这个余数就是对应key的槽位。

每个槽位都会唯一映射到后面的多个Redis实例之一，Codis会在内存中维护槽位和Redis实例的映射关系。这样有了上面key对应的槽位，那么它应该转发到哪个Redis实例就明确了。

槽位数量默认是1024，它是可以设置的，如果集群节点比较多，建议将这个数值设置大一些，比如2048/4096等。

## 不同Codis实例之间槽位关系如何同步

如果Codis的槽位映射关系只存储在内存中，那么不同的Codis实例之间的槽位关系就无法得到同步。所以Codis还需要一个分布式配置存储数据库专门用来持久化槽位关系。Codis开始使用zookeeper，后来连etcd也一块支持了。Codis将槽位关系存储在zookeeper中，并且提供了一个Dashboard可以用来观察和修改槽位关系，当槽位关系变化时，Codis Proxy会监听到变化并重新同步槽位关系，从而实现多个Codis Proxy之间共享相同的槽位关系配置。

## Codis扩容

刚开始Codis后端只有一个Redis实例，1024个槽位全部指向同一个Redis。然后一个Redis实例内存不够了，所以又加了一个Redis实例。这时候需要对槽位关系进行调整，将一半的槽位划分到新的节点。这意味着需要对这一半槽位对应的所有key进行迁移，迁移到新的Redis实例。

那么Codis如何找到槽位对应的所有key呢？

Codis对Redis进行了改造，增加了SLOTSSCAN指令，可以遍历指定slot下所有的key。Codis通过SLOTSSCAN扫描出待迁移槽位的所有key，然后挨个迁移每个key到新的Redis节点。

在迁移过程中，Codis还是会接收到新的请求打在当前正在迁移的槽位上，因为当前槽位的数据同时存在于新旧两个槽位中，Codis如何判断该将请求转发到哪个具体实例呢？

Codis无法判定迁移过程中的key究竟在哪个实例中，所以它采用了另一种完全不同的思路。当Codis接受到位于正在迁移槽位的key后，会立即强制对当前的单个key进行迁移，迁移完成后，再将请求转发到新的Redis实例。我们知道Redis支持的所有Scan指令都是无法避免重复的，同样Codis自定义的SLOTSSCAN也是一样，但是这并不会影响迁移。因为单个key被迁移一次后，在旧实例中它被彻底删除了，也就不可能会被再次扫描出来了。

## Codis优点

Codis在设计上比Redis Cluster官方集群方案要简单很多，因为它将分布式的问题交给了第三方（zookeeper或etcd）去负责，自己就省去了复杂的分布式一致性代码的编写维护工作。而与之相比，Redis Cluster的内部实现非常复杂，它为了实现去中心化，混合使用了复杂的Raft和Gossip协议，还要大量的 需要调优的配置参数，当集群出现故障时，维护人员往往不知道从何处着手。

## two

Codis 是一个分布式 Redis 解决方案，其目标是提供一个稳定、高可用和可扩展的 Redis 服务。在 Codis 的架构中，数据被分布在多个 Redis 实例上，通过 Codis Proxy 进行路由和转发，从而实现水平扩展和高可用性。

Codis 的主要特点如下：

1. 水平扩展：Codis 支持将数据分布到多个 Redis 实例上，通过 Codis Proxy 进行路由和转发，从而实现水平扩展。当某个 Redis 实例负载过高时，可以通过添加新的 Redis 实例来分担负载。
2. 高可用性：Codis 提供了一致性哈希算法，当某个 Redis 实例出现故障时，Codis 可以自动将数据重新分布到其他健康的 Redis 实例上，保证服务的可用性。
3. 自动数据迁移：Codis 支持在不停止服务的情况下进行数据迁移，保证了服务的连续性和稳定性。
4. 支持多种数据结构：Codis 支持 Redis 支持的所有数据结构，包括字符串、列表、集合、有序集合等。
5. 丰富的特性：Codis 还提供了许多 Redis 原生不支持的特性，例如慢查询分析、日志记录、性能监控等。

总的来说，Codis 是一个功能强大、易于使用的分布式 Redis 解决方案，适用于需要大规模存储和处理的场景。

# Pika

## one

Pika 是一个 Python 编程语言的 Redis 客户端库，它提供了与 Redis 服务器进行交互的功能。下面是关于 Pika 的详细介绍：

特点：
1. 简单易用：Pika 提供了简单而直观的 API，使得开发者可以轻松地与 Redis 服务器进行通信。它提供了一组易于理解和使用的方法，用于执行常见的 Redis 操作，例如设置键值、获取键值、发布订阅等。

2. 异步支持：Pika 支持异步操作，可以与异步框架（如 asyncio、Tornado、Twisted 等）结合使用。这使得在高并发场景下能够更好地利用系统资源，并提高性能和响应速度。

3. 支持管道操作：Pika 允许使用 Redis 的管道（pipeline）功能，通过将多个命令打包成一个请求同时发送给 Redis 服务器，从而减少了网络往返的延迟，并提高了性能。

4. 支持事务操作：Pika 提供了对 Redis 事务的支持，可以通过 MULTI、EXEC、WATCH 等命令实现事务的原子性操作。这允许开发者在执行多个 Redis 命令时保持原子性，确保这些命令要么全部执行成功，要么全部失败。

5. 高可靠性：Pika 实现了与 Redis 服务器的连接池管理，可以自动管理连接的建立和释放，以及连接的重连机制。这确保了在面对网络故障或 Redis 服务器重启时，Pika 能够自动恢复连接，并提供可靠的操作。

6. 兼容性：Pika 兼容不同版本的 Redis，包括支持 Redis 的主从复制、哨兵模式和集群模式。它支持密码验证、TLS/SSL 加密等安全特性，并提供了灵活的配置选项，以适应各种不同的部署需求。

使用示例：
下面是一个简单的使用 Pika 进行 Redis 操作的示例：

```python
import pika

# 创建一个 Redis 连接
connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost', port=6379))

# 创建一个 Redis 客户端
client = connection.channel()

# 执行 Redis 操作
client.set('key', 'value')
result = client.get('key')
print(result)

# 关闭连接
connection.close()
```

总结：
Pika 是一个简单易用的 Python Redis 客户端库，提供了与 Redis 服务器进行交互的功能。它支持简单操作、管道操作、事务操作等 Redis 特性，具有异步支持和高可靠性。通过 Pika，开发者可以方便地在 Python 程序中使用 Redis，并构建出高性能和可靠的应用程序。

## two

Pika 是 360 DBA 和基础架构组联合开发的类 Redis 存储系统，旨在解决 Redis 在大规模使用时遇到的一些问题。

Pika 主要使用了持久化存储，解决了 Redis 在内存占用超过一定大小时，如启动恢复时间长、主从同步代价大、硬件成本贵等问题。此外，Pika 完全支持 Redis 协议，用户不需要修改任何代码，就可以将服务迁移至 Pika。Pika 兼容了 Redis 的大部分命令（如 String、Hash、List、ZSet、Set），并且用户在使用上与使用 Redis 基本无感知。

此外，Pika 还支持像 Redis 一样的主从备份功能，可以进行全同步和部分同步。同时，360 DBA 团队还提供了迁移工具，使得用户在迁移过程中不会感到任何不适应，迁移过程非常平滑。

总的来说，Pika 是一个与 Redis 兼容的存储系统，主要解决了 Redis 在大规模使用时遇到的问题，并且用户在使用上与使用 Redis 基本无感知。

## 面试题

### 什么是Redis哨兵模式

- Redis哨兵模式是一种用于监控和管理Redis实例的解决方案。它由一个或多个哨兵节点组成，哨兵节点通过相互通信来监控Redis实例的状态，并在主节点发生故障时自动将从节点升级为主节点。

### Redis哨兵模式的原理

- Redis哨兵模式的原理是通过哨兵节点之间的相互通信来实现的。每个哨兵节点会定期向其他哨兵节点发送PING命令，如果一个哨兵节点发现主节点不可用，它会向其他哨兵节点发送选举消息，最终哨兵节点会协商选择一个从节点升级为新的主节点。

### Redis哨兵模式的优点

- 自动故障转移：当主节点发生故障时，哨兵节点可以自动将一个从节点升级为新的主节点，从而实现故障转移，保证系统的可用性。
- 主节点选举：哨兵节点可以在主节点不可用时，协商选举一个新的主节点，保证系统的持续可用性。
- 配置简单：哨兵模式相对于主从复制来说，配置更加简单，只需要配置哨兵节点的信息即可。

### Redis哨兵模式的缺点

- 响应时间较长：由于哨兵节点之间需要相互通信来检测和管理Redis实例，因此在故障转移和主节点选举过程中可能会有一定的延迟，导致系统的响应时间变长。
- 单点故障：哨兵模式仍然存在单点故障问题，当所有的哨兵节点都不可用时，系统将无法进行故障转移和主节点选举。

### 哨兵模式的配置步骤

- 配置哨兵节点：在哨兵节点的配置文件中设置监控的Redis实例信息，并启动哨兵节点。
- 配置Redis实例：在Redis实例的配置文件中设置哨兵模式的相关参数，并启动Redis实例。
- 验证哨兵模式：可以通过停止主节点的Redis实例，观察哨兵节点是否能够自动将从节点升级为新的主节点来验证哨兵模式的可用性。

## redis怎么设计高扩展框架

设计高扩展架构的Redis可以采用以下几个关键策略：

1. 数据分片：将数据分散存储在多个Redis节点上，以减轻单个节点的负载。可以使用一致性哈希算法或分片算法来决定数据应该存储在哪个节点上。这样可以水平扩展Redis集群，增加节点数量来处理更多的数据和请求。

2. 主从复制：使用Redis的主从复制功能，将主节点的数据复制到多个从节点上。主节点负责写操作，而从节点负责读操作。这样可以提高读取性能并提供高可用性。如果主节点发生故障，可以将其中一个从节点升级为主节点，以确保系统的连续性。

3. 哨兵模式：使用Redis的哨兵模式来实现主从节点的自动故障检测和故障转移。哨兵是一个独立的进程，监视Redis节点的状态。当主节点发生故障时，哨兵会自动将一个从节点升级为新的主节点，并通知其他从节点更新配置。

4. 集群模式：Redis 3.0及更高版本提供了集群模式，可以将数据分布在多个节点上，并自动进行数据分片和故障转移。Redis集群使用哈希槽（hash slot）来分配数据，并将哈希槽均匀分配给不同的节点。集群模式提供了高可用性和水平扩展的能力。

5. 缓存与持久化结合：将Redis配置为将热数据存储在内存中，而将冷数据存储在持久化存储（如磁盘）中。这样可以在保证性能的同时，确保数据的持久性和可恢复性。

6. 使用代理：在前端引入代理层，如Twemproxy或Redis Cluster Proxy，以负责将请求路由到正确的Redis节点。代理可以根据数据的键或哈希值将请求转发到相应的节点，从而实现负载均衡和请求分发。

7. 监控和自动化：使用监控工具来实时监测Redis集群的状态和性能指标，如内存使用情况、吞吐量和延迟等。通过监控数据，可以对集群进行动态调整和优化。此外，自动化工具可以帮助处理节点故障、重新分片和扩展等操作，提高系统的可靠性和可维护性。

这些策略可以根据具体需求进行组合和调整，以设计出适合高扩展性的Redis架构。

# redis单节点不够，怎么设计扩容

当Redis单节点的容量不足以处理负载时，可以采取以下几种方式进行扩容：

1. 垂直扩展：通过升级硬件资源来增加Redis单节点的容量。可以将节点部署在更高性能的服务器上，增加CPU、内存等硬件资源，以提高Redis的处理能力。这种方式的扩容相对简单，但受限于硬件资源的极限，无法无限制地扩展。
2. 主从复制：通过将主节点的数据复制到多个从节点上，可以实现读操作的负载均衡。从节点可以处理读取请求，从而减轻主节点的负载。这种方式可以提高读取性能，并且具有高可用性，因为如果主节点故障，可以将其中一个从节点升级为主节点。
3. 数据分片：将数据分散存储在多个Redis节点上，以提高整个集群的处理能力。可以使用一致性哈希算法或分片算法来决定数据应该存储在哪个节点上。这样可以水平扩展Redis集群，增加节点数量来处理更多的数据和请求。
4. Redis集群模式：Redis 3.0及更高版本提供了集群模式，可以将数据分布在多个节点上，并自动进行数据分片和故障转移。集群模式提供了高可用性和水平扩展的能力。可以通过添加新的节点来扩展集群容量，并自动将数据进行重新分片。
5. 代理层：在前端引入代理层，如Twemproxy或Redis Cluster Proxy，以负责将请求路由到正确的Redis节点。代理可以根据数据的键或哈希值将请求转发到相应的节点，从而实现负载均衡和请求分发。通过添加更多的代理节点，可以实现扩展和负载均衡。

无论采用哪种方式，扩容Redis都需要考虑数据迁移、负载均衡、故障恢复等因素。在进行扩容操作之前，建议进行充分的规划和测试，以确保系统的平稳扩展和高可用性。

# 一致性哈希算法

一致性哈希算法是一种常用的分布式哈希算法，用于在节点动态增减的环境下，将数据均匀地分布到不同的节点上。它可以有效地解决节点数量变化时数据迁移的问题，同时提供了负载均衡和高可用性。

下面是一致性哈希算法的基本原理和步骤：

1. 环形空间：将哈希值的范围映射到一个环形空间中，通常使用32位或64位的整数表示。

2. 节点映射：将每个节点通过哈希函数映射到环形空间的一个点上。节点可以是物理服务器、虚拟节点或存储服务等。

3. 数据定位：对于要存储的数据，通过哈希函数计算其哈希值，并在环形空间中查找离该哈希值最近的节点。从该节点开始，顺时针查找，直到找到第一个节点，将数据存储在该节点上。

4. 节点动态变化：当节点数量发生变化时，如节点添加或移除，只需对受影响的数据进行重新映射。对于新增节点，可以在环形空间上插入其映射点，并将其周围的数据重新定位到新节点。对于删除节点，需要将其映射点从环形空间中移除，并将其负责的数据重新定位到其他节点。

5. 均衡性：一致性哈希算法通过将数据尽可能均匀地分布在环形空间上，使得数据在节点上的分布相对平衡。这样可以减少因节点变化而引起的数据迁移量，提高系统的稳定性和性能。

6. 虚拟节点：为了提高均衡性，可以使用虚拟节点（也称为虚拟副本）来表示一个物理节点。通过为每个物理节点创建多个虚拟节点，可以使得节点在环形空间上的映射更加分散，从而增加数据的均衡性。

一致性哈希算法的优点在于节点动态变化时的数据迁移量相对较小，可以保持系统的稳定性和性能。它被广泛应用于分布式缓存、负载均衡、分布式数据库等场景，以提供高可用性和水平扩展的能力。

# redis分片集群，如何分片的，有什么好处

Redis 分片集群是将数据分散存储在多个 Redis 节点上的技术，通过将数据划分为多个分片（shard）并分布在不同的节点上，可以实现水平扩展和提高整体性能。下面是 Redis 分片集群的分片策略和好处：

分片策略：
1. 哈希分片：根据键的哈希值将数据分配到不同的分片上。通常使用哈希函数计算键的哈希值，并根据哈希值的范围将数据分发到对应的分片。

2. 范围分片：将数据划分为连续的范围，然后将每个范围分配给不同的分片。例如，可以根据键的字母顺序或数字范围将数据分片。

好处：
1. 水平扩展：通过将数据分布在多个节点上，Redis 分片集群可以实现水平扩展，允许存储更多的数据和处理更多的请求。新的节点可以简单地加入集群，从而增加整体容量和性能。

2. 负载均衡：通过均匀分布数据到不同的分片上，Redis 分片集群可以实现负载均衡。请求可以在不同的节点上并行处理，从而提高整体的处理能力和响应速度。

3. 容错性：由于数据被复制到多个节点上，当某个节点发生故障时，数据仍然可用于其他节点。Redis 分片集群可以通过数据复制和故障转移来提供高可用性和容错性。

4. 灵活性：Redis 分片集群允许根据实际需求动态添加或删除节点，从而调整集群的容量和性能。这种灵活性使得集群可以根据实时需求进行扩缩容，适应变化的负载。

需要注意的是，Redis 分片集群也带来了一些挑战，例如跨分片事务的处理、数据迁移和节点故障恢复等。在设计和使用 Redis 分片集群时，需要综合考虑这些因素，并选择合适的分片策略来满足应用的需求。
