# 四层与七层负载均衡的对比

四层负载均衡基于IP和端口的方式实现网络的负载均衡，具体实 现为对外提供一个虚拟IP和端口接收所有用户的请求，然后根据负载均衡配置和负载均衡策略将请求发送给真实的服务器。

七层负载均衡基于URL等资源来实现应用层基于内容的负载均衡， 具体实现为通过虚拟的URL或主机名接收所有用户的请求，然后将请求发送给真实的服务器。

四层负载均衡和七层负载均衡的最大差别是：四层负载均衡只能 针对IP地址和端口上的数据做统一的分发，而七层负载均衡能根据消 息的内容做更加详细的有针对性的负载均衡。我们通常使用LVS等技术 实现基于Socket的四层负载均衡，使用Nginx等技术实现基于内容分发 的七层负载均衡，比如将以“/user/ * * * ”开头的URL请求负载到单点 登录服务器，而将以“/business/* * * ”开头的URL请求负载到具体的业务服务器，如图所示。

![image-20240304084448017](D:\typora\Golang_Engineer\typora-user-images\image-20240304084448017.png)

## 四层负载均衡

四层负载均衡主要通过修改报文中的目标地址和端口来实现报文的分发和负载均衡。以TCP为例，负载均衡设备在接收到第1个来自客户端的SYN请求后，会根据负载均衡配置和负载均衡策略选择一个最佳 的服务器，并将报文中的目标IP地址修改为该服务器的IP直接转发给 该服务器。TCP连接的建立（即三次握手过程）是在客户端和服务器端之间完成的，负载均衡设备只起到路由器的转发功能。

四层负载均衡常用的软硬件如下。

- F5：硬件负载均衡器，功能完备，价格昂贵。
- LVS：基于IP+端口实现的四层负载软件，常和Keepalive配合使用。
- Nginx：同时实现四层负载和七层负载均衡，带缓存功能，可基于正则表达式灵活转发。

## 七层负载均衡

七层负载均衡又叫作“内容负载均衡”，主要通过解析报文中真正有意义的应用层内容，并根据负载均衡配置和负载均衡策略选择一个最佳的服务器响应用户的请求。七层应用负载可以使整个网络更智能化，七层负载均衡根据不同的数据类型将数据存储在不同的服务器上来提高网络整体的负载能力。比如将客户端的基本信息存储在内存较大的缓存服务器上，将文件信息存储在磁盘空间较大的文件服务器上，将图片视频存储在网络I/O能力较强的流媒体服务器上。在接收到不同的客户端的请求时从不同的服务器上获取数据并将其返回给客户端，提高客户端的访问效率

七层负载均衡常用的软件如下：

- HAProxy：支持七层代理、会话保持、标记、路径转移等。
- Nginx：同时实现四层负载和七层负载均衡，在HTTP和Mail协议上功能比较好，性能与HAProxy差不多。
- Apache：使用简单，性能较差。

# 负载均衡算法

常用的负载均衡算法有：轮询均衡（Round Robin）、权重轮询均 衡（Weighted Round Robin）、随机均衡（Random）、权重随机均衡（Weighted Random）、响应速度均衡（Response Time）、最少连接数均衡（Least Connection）、处理能力均衡、DNS响应均衡（Flash DNS）、散列算法均衡、IP地址散列、URL散列。不同的负载均衡算法适用于不同的应用场景。

## 轮询均衡

轮询均衡指将客户端请求轮流分配到 1至 N台服务器上，每台服务器均被均等地分配一定数量的客户端请求。轮询均衡算法适用于集群中所有服务器都有相同的软硬件配置和服务能力的情况下。

## 权重轮询均衡

权重轮询均衡指根据每台服务器的不同配置及服务能力，为每台服务器都设置不同的权重值，然后按照设置的权重值轮询地将请求分配到不同的服务器上。例如，服务器A的权重值被设计成 3，服务器B的权重值被设计成 3，服务器C的权重值被设计成 4，则服务器A、B、C将分别承担 30%、30%、40%的客户端请求。权重轮询均衡算法主要用于服务器配置不均等的集群中。

## 随机均衡

随机均衡指将来自网络的请求随机分配给内部的多台服务器，不考虑服务器的配置和负载情况。

## 权重随机均衡

权重随机均衡算法类似于权重轮询算法，只是在分配请求时不再轮询发送，而是随机选择某个权重的服务器发送。

## 响应速度均衡

响应速度均衡指根据服务器设备响应速度的不同将客户端请求发送到响应速度最快的服务器上。对响应速度的获取是通过负载均衡设备定时为每台服务都发出一个探测请求（例如Ping）实现的。响应速度均衡能够为当前的每台服务器根据其不同的负载情况分配不同的客户端请求，这有效避免了某台服务器单点负载过高的情况。但需要注意的是，这里探测到的响应速度是负载均衡设备到各个服务器之间的响应速度，并不完全代表客户端到服务器的响应速度，因此存在一定偏差。

## 最少连接数均衡

最少连接数均衡指在负载均衡器内部记录当前每台服务器正在处理的连接数量，在有新的请求时，将该请求分配给连接数最少的服务器。这种均衡算法适用于网络连接和带宽有限、CPU处理任务简单的请求服务，例如FTP。

## 处理能力均衡

处理能力均衡算法将服务请求分配给内部负荷最轻的服务器，负荷是根据服务器的CPU型号、CPU数量、内存大小及当前连接数等换算而成的。处理能力均衡算法由于考虑到了内部服务器的处理能力及当前网络的运行状况，所以相对来说更加精确，尤其适用于七层负载均衡的场景。

## DNS响应均衡

DNS响应均衡算法指在分布在不同中心机房的负载均衡设备都收到同一个客户端的域名解析请求时，所有负载均衡设备均解析此域名并将解析后的服务器IP地址返回给客户端，客户端向收到第一个域名解析后的IP地址发起请求服务，而忽略其他负载均衡设备的响应。这种均衡算法适用于全局负载均衡的场景。

## 散列算法均衡

散列算法均衡指通过一致性散列算法和虚拟节点技术将相同参数的请求总是发送到同一台服务器，该服务器将长期、稳定地为某些客户端提供服务。在某个服务器被移除或异常宕机后，该服务器的请求基于虚拟节点技术平摊到其他服务器，而不会影响集群整体的稳定性。、

## IP地址散列

IP地址散列指在负载均衡器内部维护了不同链接上客户端和服务器的IP对应关系表，将来自同一客户端的请求统一转发给相同的服务器。该算法能够以会话为单位，保证同一客户端的请求能够一直在同一台服务器上处理，主要适用于客户端和服务器需要保持长连接的场景，比如基于TCP长连接的应用。

## URL散列

URL散列指通过管理客户端请求URL信息的散列表，将相同URL的请求转发给同一台服务器。该算法主要适用于在七层负载中根据用户请求类型的不同将其转发给不同类型的应用服务器。URL散列指通过管理客户端请求URL信息的散列表，将相同URL的请求转发给同一台服务器。该算法主要适用于在七层负载中根据用户请求类型的不同将其转发给不同类型的应用服务器

# LVS原理及应用

LVS（Linux Virtual Server）是一个虚拟的服务器集群系统，采用IP负载均衡技术将请求均衡地转移到不同的服务器上执行，且通过调度器自动屏蔽故障服务器，从而将一组服务器构成一个高性能、高可用的虚拟服务器。整个服务器集群的结构对用户是透明的，无须修改客户端和服务器端的程序，便可实现客户端到服务器的负载均衡。

## LVS的原理

LVS由前端的负载均衡器（Load Balancer，LB）和后端的真实服务器（Real Server，RS）群组成，在真实服务器间可通过局域网或广域网连接。LVS的这种结构对用户是透明的，用户只需要关注作为LB的虚拟服务器（Virtual Server），而不需要关注提供服务的真实服务器群。在用户的请求被发送给虚拟服务器后，LB根据设定的包转发策略和负载均衡调度算法将用户的请求转发给真实服务器，真实服务器再将用户请求的结果返回给用户。

实现LVS的核心组件有负载均衡调度器、服务器池和共享存储。

- 负载均衡调度器（Load Balancer/Director）：是整个集群对外提供服务的入口，通过对外提供一个虚拟IP 来接收客户端请求。在客户端将请求发送到该虚拟IP 后，负载均衡调度器会负责将请求按照负载均衡策略发送到一组具体的服务器上。
- 服务器池（Server Pool）：服务器池是一组真正处理客户端请求的真实服务器，具体执行的服务有WEB、MAIL、FTP和DNS等。
- 共享存储（Shared Storage）：为服务器池提供一个共享的存储区，使得服务器池拥有相同的内容，提供相同的服务。

在接收LVS内部数据的转发流程前，这里先以表 6-3介绍LVS技术中常用的一些名词，以让我们更好地理解LVS的工作原理。

![image-20240304085623255](D:\typora\Golang_Engineer\typora-user-images\image-20240304085623255.png)LVS的IP负载均衡技术是通过IPVS模块实现的。IPVS是LVS集群系统 的 核 心 软 件 ， 被 安 装 在 Director Server 上 ， 同 时 在 DirectorServer上虚拟出一个IP地址。用户通过这个虚拟的IP地址访问服务器。这个虚拟的IP地址一般被称为LVS的VIP，即Virtual IP。访问的请求首先经过VIP到达负载调度器，然后由负载调度器从真实服务器列表中选取一个服务节点响应用户的请求。

## LVS数据转发

LVS的数据转发流程是LVS设计的核心部分，如下所述，如图所示。

1. PREROUTING链接收用户请求：客户端向PREROUTING链发送请求。
2. INPUT链转发：在PREROUTING链通过RouteTable列表发现请求数据包的目的地址是本机时，将数据包发送给INPUT链。
3. IPVS检查：IPVS检查INPUT链上的数据包，如果数据包中的目的地址和端口不在规则列表中，则将该数据包发送到用户空间的ipvsadm。ipvsadm主要用于用户定义和管理集群。
4. POSTROUTING链转发：如果数据包里面的目的地址和端口都在规则里面，那么将该数据包中的目的地址修改为事先定义好的真实服务器地址，通过FORWARD将数据发送到POSTROUTING链。
5. 真实服务器转发：POSTROUTING链根据数据包中的目的地址将数据包转发到真实服务器。

![image-20240304085703600](D:\typora\Golang_Engineer\typora-user-images\image-20240304085703600.png)

## LVS NAT模式

LVS NAT（Network Address Translation）即网络地址转换模式，具体的实现流程如图所示。



![image-20240304085735884](D:\typora\Golang_Engineer\typora-user-images\image-20240304085735884.png)

LVS NAT模式

NAT模式通过对请求报文和响应报文的地址进行改写完成对数据的转发，具体流程如下。

1. 客户端将请求报文发送到LVS，请求报文的源地址是CIP（Client IP Address，客户端IP），目标地址是VIP（Virtual IP Address，虚拟IP）。
2. LVS在收到报文后，发现请求的IP地址在LVS的规则列表中存 在，则将客户端请求报文的目标地址VIP修改为RIP（Real-server IP Address，后端服务器的真实IP），并将报文发送到具体的真实服务器上。
3. 真实服务器在收到报文后，由于报文的目标地址是自己的IP，所以会响应该请求，并将响应报文返回给LVS。
4. LVS在收到数据后将此报文的源地址修改为本机IP地址，即VIP，并将报文发送给客户端。

NAT模式的特点如下:

- 请求的报文和响应的报文都需要通过LVS进行地址改写，因此在并发访问量较大的时候LVS存在瓶颈问题，一般适用于节点不是很多的情况下。
- 只需要在LVS上配置一个公网IP即可。
- 每台内部的真实服务器的网关地址都必须是LVS的内网地址。
- NAT 模式支持对IP 地址和端口进行转换，即用户请求的端口和真实服务器的端口可以不同。

##  LVS DR模式

LVS DR（Direct Routing）模式用直接路由技术实现，通过改写请求报文的MAC地址将请求发送给真实服务器，具体的实现流程如下图所示。

![image-20240304085819894](D:\typora\Golang_Engineer\typora-user-images\image-20240304085819894.png)LVD DR模式是局域网中经常被用到的一种模式，其报文转发流程如下。

1. 客户端将请求发送给LVS，请求报文的源地址是CIP，目标地址是VIP。
2. LVS在收到报文后，发现请求在规则中存在，则将客户端请求报文的源MAC地址改为自己的DIP（Direct IP Address，内部转发IP）的MAC地址，将目标MAC改为RIP的MAC地址，并将此包发送给真实服务器。
3. 真实服务器在收到请求后发现请求报文中的目标MAC是自己，就会将此报文接收下来，在处理完请求报文后，将响应报文通过lo（回环路由）接口发送给eth0网卡，并最终发送给客户端。

NAT模式的特点如下：

- 通过LVS修改数据包的目的MAC地址实现转发。注意，源IP地址仍然是CIP，目标IP地址仍然是VIP地址。
- 请求的报文均经过LVS，而真实服务器响应报文时无须经过LVS，因此在并发访问量大时比NAT模式的效率高很多。
- 因为DR 模式是通过MAC 地址改写机制实现转发的，因此所有真实服务器节点和LVS只能被部署在同一个局域网内。
- 真实服务器主机需要绑定VIP 地址在lo接口（掩码 32 位）上，并且需要配置ARP抑制。
- 真实服务器节点的默认网关无须被配置为LVS网关，只需要被配置为上级路由的网关，能让真实服务器直接出网即可。
- DR 模式仅做MAC 地址的改写，不能改写目标端口，即真实服务器端口和VIP端口必须相同。

## LVS TUN模式

TUN（IP Tunneling）通过IP隧道技术实现，具体的实现流程如下图所示。

LVS TUN模式常用于跨网段或跨机房的负载均衡，具体的报文转发流程如下。

1. 客户端将请求发送给前端的LVS，请求报文的源地址是CIP，目标地址是VIP。
2. LVS在收到报文后，发现请求在规则里中存在，则将在客户 端请求报文的首部再封装一层IP报文，将源地址改为DIP，将目标地址改为RIP，并将此包发送给真实服务器。

![image-20240304085856526](D:\typora\Golang_Engineer\typora-user-images\image-20240304085856526.png)



3.真实服务器在收到请求报文后会先拆开第1层封装，因为发 现里面还有一层IP首部的目标地址是自己lo接口上的VIP，所以会处理该请求报文，并将响应报文通过lo接口发送给eth0网卡，并最终发送给客户端。

TUN模式的特点如下：

- UNNEL模式需要设置lo接口的VIP不能在公网上出现。
- TUNNEL模式必须在所有的真实服务器上绑定VIP的IP地址。
- TUNNEL 模式中VIP→真实服务器的包通信通过TUNNEL 隧道技术实现，不管是内网还是外网都能通信，所以不需要LVS和真实服务器在同一个网段内。
- 在TUNNEL 模式中，真实服务器会把响应报文直接发送给客户端而不经过LVS，负载能力较强。
- TUNNEL 模式采用的是隧道模式，使用方法相对复杂，一般用于跨机房LVS 实现，并且需要所有服务器都支持IP Tunneling或IPEncapsulation协议。

## LVS FULLNAT模式

无论是DR模式还是NAT模式，都要求LVS和真实服务器在同一个VLAN下，否则LVS无法作为真实服务器的网关，因此跨VLAN的真实服务器无法接入。同时，在流量增大、真实服务器水平扩容时，单点LVS会成为瓶颈。

FULLNAT能够很好地解决LVS和真实服务器跨VLAN的问题，在跨VLAN问题解决后，LVS和真实服务器不再存在VLAN上的从属关系，可以做到多个LVS对应多个真实服务器，解决水平扩容的问题。FULLNAT的原理是在NAT的基础上引入Local Address IP（内网IP地址），将CIP→VIP转换为LIP→RIP，而LIP和RIP均为IDC内网IP，可以通过交换机实现跨VLAN通信。FULLNAT的具体实现流程如图所示。

![image-20240304085937796](D:\typora\Golang_Engineer\typora-user-images\image-20240304085937796.png)

LVS FULLNAT具体的报文转发流程如下:

1. 客户端将请求发送给LVS的DNAT，请求报文的源地址是CIP，目标地址是VIP。
2. LVS 在 收 到 数 据 后 将 源 地 址 CIP 修 改 成 LIP （ Local IP Address，LVS的内网IP），将目标地址VIP修改为RIP，并将数据发送到真实服务器。多个LIP在同一个IDC数据中心，可以通过交换机跨VLAN通信。
3. 真实服务器在收到数据包并处理完成后，将目标地址修改为LIP，将源地址修改为RIP，最终将这个数据包返回给LVS。
4. LVS在收到数据包后，将数据包中的目标地址修改为CIP，将源地址修改为VIP，并将数据发送给客户端。

# Nginx反向代理与负载均衡

一般的负载均衡软件如LVS实现的功能只是对请求数据包的转发和传递，从负载均衡下的节点服务器来看，接收到的请求还是来自访问负载均衡器的客户端的真实用户；而反向代理服务器在接收到用户的访问请求后，会代理用户重新向节点服务器（Web服务器、文件服务器、视频服务器）发起请求，反向代理服务器和节点服务器做具体的数据交互，最后把数据返回给客户端用户。在节点服务器看来，访问的节点服务器的客户端就是反向代理服务器，而非真实的网站访问用户，具体原理如图所示。

![image-20240304090013909](D:\typora\Golang_Engineer\typora-user-images\image-20240304090013909.png)

##  1.upstream_module

ngx_http_upstream_module是Nginx的负载均衡模块，可以实现网站的负载均衡功能即节点的健康检查。upstream模块允许Nginx定义一组或多组节点服务器，在使用时可通过proxy_pass代理方式把网站的请求发送到事先定义好的对应Upstream组的名字上。具体的upstream定义如下：

```bash
upstream restLVSServer{
	server 191.168.1.10:9000 weight=5 ;
	server 191.168.1.11:9000;
	server example.com:9000 max_fails=2 fail_timeout=10s backup;
}
```

如上代码定义了名为restLVSServer的upstream，并在其中定义了3个服务地址，在用户请求restLVSServer服务时，Nginx会根据权重将请求转发到具体的服务器。常用的upstream配置如下。

- weight：服务器权重。
- max_fails：Nginx尝试连接后端服务器的最大失败次数，如果失败时大于max_fails，则认为该服务器不可用。
- fail_timeout：max_fails和fail_timeout一般会关联使用，如果某台服务器在fail_timeout时间内出现了max_fails次连接失败，那么Nginx会认为其已经挂掉，从而在fail_timeout时间内不再去请求它，fail_timeout默认是 10s，max_fails默认是1，即在默认情况下只要发生错误就认为服务器挂了，如果将max_fails设置为0，则表示取消这项检查。
- backup：表示当前服务器是备用服务器，只有其他非backup后端服务器都挂掉或很忙时，才会分配请求给它。
- down：标志服务器永远不可用。

##  2.proxy_pass

proxy_pass指令属于ngx_http_proxy_module模块，此模块可以将请求转发到另一台服务器，在实际的反向代理工作中，会通过location功能匹配指定的URI，然后把接收到的服务匹配URI的请求通过proxy_pass抛给定义好的upstream节点池。具体的proxy_pass定义如下：

```bash
location /download/ {
	proxy_pass http://192.168.1.13:9000/download/vedio/;
}
```

如 上 代 码 定 义 了 一 个 download 的 反 向 代 理 ， 在 客 户 端 请求/download时，Nginx会将具体的请求转发给proxy_pass配置的地址处理请求，这里配置的地址是http://192.168.1.13:9000/download/vedio/。常用的proxy_pass配置如表所示。

![image-20220729093931993](https://image-1302243118.cos.ap-beijing.myqcloud.com/java/image-20220729093931993.png)